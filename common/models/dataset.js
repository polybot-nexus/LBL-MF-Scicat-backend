"use strict";

const kafka = require("kafka-node");

var config = require("../../server/config.local");
var p = require("../../package.json");
var utils = require("./utils");
const lodash = require("lodash");
const logger = require("../logger");
// TODO Feature  Add delete functionality for dataset, which removes Dataset and all linked data:
// OrigDatablock and Datablock and DatasetAttachments

module.exports = function(Dataset) {
  var app = require("../../server/server");

  Dataset.appendToArrayField = function(id, fieldName, data, ctx, next) {
    const where = { pid: id };
    var $addToSet = {};
    // $each is necessary as data is an array of values
    // $addToSetis necessary to append to the field and not overwrite
    $addToSet[fieldName] = { $each: data };
    Dataset.update(where, { $addToSet });
    next();
  };


  Dataset.prototype.updateSize = function(id, sizeField, size, numFilesField, numFiles, next) {
    // console.log("Updating size field:", id, sizeField, size)
    Dataset.findById(id, function(err, instance) {
      if (err) {
        return next(err);
      } else {
        // console.log("Before addition:",sizeField,instance[sizeField])
        var oldsize = 0;

        if (instance[sizeField]) {
          oldsize = instance[sizeField];
        }
        instance[sizeField] = size + oldsize;
        // console.log("new size:",instance[sizeField])
        var oldNumFiles=0;
        if (instance[numFilesField]) {
          oldNumFiles = instance[numFilesField];
        }
        instance[numFilesField] = numFiles + oldNumFiles;

        instance.save();
        return next();

      }
    });
  };

  Dataset.validatesUniquenessOf("pid");

  // put
  Dataset.beforeRemote("replaceOrCreate", function(ctx, instance, next) {
    // console.log("++++++++++++ PUT")
    utils.updateTimesToUTC(["creationTime"], ctx.args.data);
    utils.dropAutoGeneratedFields(ctx.args.data, next);
  });

  // patch
  Dataset.beforeRemote("patchOrCreate", function(ctx, instance, next) {
    // console.log("+++++++++++++++ PATCH")
    utils.updateTimesToUTC(["creationTime"], ctx.args.data);
    utils.dropAutoGeneratedFields(ctx.args.data, next);
  });

  // post
  Dataset.beforeRemote("create", function(ctx, unused, next) {
    // console.log("+++++++++++++++ POST")
    utils.updateTimesToUTC(["creationTime"], ctx.args.data);
    utils.dropAutoGeneratedFields(ctx.args.data, next);
  });

  Dataset.beforeRemote("find", function(ctx, unused, next) {
    const accessToken = ctx.args.options.accessToken;
    if (!accessToken) {
      if (!ctx.args.filter) {
        ctx.args.filter = { where: { isPublished: true } };
      } else {
        if (!ctx.args.filter.where) {
          ctx.args.filter.where = { isPublished: true };
        } else {
          ctx.args.filter.where["isPublished"] = true;
        }
      }
    }
    next();
  });

  Dataset.afterRemote("findById", function(ctx, unused, next) {
    const accessToken = ctx.args.options.accessToken;
    let error;
    if (!accessToken) {
      if (ctx.result && !ctx.result.isPublished) {
        error = new Error("Dataset is not public");
        error.status = 403;
      }
    }
    next(error);
  });
  Dataset.beforeRemote("prototype.__get__attachments", function(ctx, unused, next){
    checkACLtoRelatedModel(ctx, next);
  });

  Dataset.beforeRemote("prototype.__get__origdatablocks", function(ctx, unused, next){
    checkACLtoRelatedModel(ctx, next);
  });

  Dataset.beforeRemote("prototype.__get__datablocks", function(ctx, unused, next){
    checkACLtoRelatedModel(ctx, next);
  });

  const checkACLtoRelatedModel = (ctx, next) => {
    const accessToken = ctx.args.options.accessToken;
    const error = new Error("Authorization Required");
    error.statusCode = 401;
    error.name = "Error";
    error.code = "AUTHORIZATION_REQUIRED";
    const groups = ctx.args.options.currentGroups;
    // Allow access to related model if dataset is published or user is owner or user is in a group having read access to the dataset
    if (ctx.instance.isPublished
        || (accessToken
            && (groups.indexOf(ctx.instance.ownerGroup) !== -1
            || groups.indexOf("globalaccess") !== -1
            || ctx.instance.accessGroups && groups.some(g => ctx.instance.accessGroups.indexOf(g) !== -1)
            || ctx.instance.sharedWith && groups.some(g =>  ctx.instance.sharedWith.indexOf(g) !== -1)
            || groups.indexOf(ctx.instance.instrumentGroup) !== -1))) {
      return next();
    }
    return next(error);
  };

  Dataset.beforeRemote("findOne", function(ctx, unused, next) {
    const accessToken = ctx.args.options.accessToken;
    if (!accessToken) {
      if (!ctx.args.filter) {
        ctx.args.filter = { where: { isPublished: true } };
      } else {
        if (!ctx.args.filter.where) {
          ctx.args.filter.where = { isPublished: true };
        } else {
          ctx.args.filter.where["isPublished"] = true;
        }
      }
    }
    next();
  });

  Dataset.beforeRemote("metadataKeys", function(ctx, unused, next) {
    const accessToken = ctx.args.options.accessToken;
    if (!accessToken) {
      ctx.args.fields.isPublished = true;
    }
    next();
  });

  Dataset.beforeRemote("fullfacet", function(ctx, unused, next) {
    const accessToken = ctx.args.options.accessToken;
    if (!accessToken) {
      ctx.args.fields.isPublished = true;
    }
    next();
  });

  Dataset.beforeRemote("fullquery", function(ctx, unused, next) {
    const accessToken = ctx.args.options.accessToken;
    if (!accessToken) {
      ctx.args.fields.isPublished = true;
    }
    next();
  });

  Dataset.afterRemote("fullquery", function (ctx, someCollections, next) {
    if (ctx.args.fields.scientific) {
      const {
        scientific
      } = ctx.args.fields;
      someCollections.forEach(({
        scientificMetadata
      }) => {
        scientific.forEach(({
          lhs,
          unit
        }) => {
          const currentUnit = lodash.get(scientificMetadata, `${lhs}.unit`);
          const currentValue = lodash.get(scientificMetadata, `${lhs}.value`);
          if (currentUnit && currentUnit !== unit) {
            const {
              valueRequested,
              unitRequested,
            } = utils.convertToRequestedUnit(
              currentValue,
              currentUnit,
              unit
            );
            lodash.update(scientificMetadata, `${lhs}.unit`, () => {return unitRequested;});
            lodash.update(scientificMetadata, `${lhs}.value`, () => {return valueRequested;});
          }
        });
      });
    }
    next();
  });

  Dataset.disableRemoteMethodByName("replaceOrCreate");

  function addDefaultPolicy(
    ownerGroup,
    accessGroups,
    ownerEmail,
    tapeRedundancy,
    ctx,
    next
  ) {
    const Policy = app.models.Policy;

    Policy.findOne(
      {
        where: {
          ownerGroup: ownerGroup
        }
      },
      function(err, policyInstance) {
        if (err) {
          return next(err);
        }
        if (policyInstance) {
          return next();
        } else {
          console.log("Adding default policy");
          const Policy = app.models.Policy;
          var defaultPolicy = Object();
          defaultPolicy.ownerGroup = ownerGroup;
          defaultPolicy.accessGroups = accessGroups;
          if (config && !ownerEmail) {
            defaultPolicy.manager = config.defaultManager;
          } else if (ownerEmail) {
            defaultPolicy.manager = ownerEmail.split(",");
          } else {
            defaultPolicy.manager = "";
          }
          if (tapeRedundancy) {
            defaultPolicy.tapeRedundancy = tapeRedundancy;
          } else {
            defaultPolicy.tapeRedundancy = "low"; // AV default low
          }
          defaultPolicy.autoArchive = false;
          defaultPolicy.autoArchiveDelay = 7;
          defaultPolicy.archiveEmailNotification = true;
          defaultPolicy.retrieveEmailNotification = true;
          defaultPolicy.archiveEmailsToBeNotified = [];
          defaultPolicy.retrieveEmailsToBeNotified = [];
          defaultPolicy.embargoPeriod = 3;
          Policy.create(defaultPolicy, ctx.options, function(
            err,
            _instance
          ) {
            if (err) {
              console.log(
                "Error when creating default policy:",
                err
              );
              return next(err);
            }
            utils.keepHistory(ctx, next);
          });
        }
      }
    );
  }

  // required so that we can reference this function from other modules (policy)
  module.exports.addDefaultPolicy = addDefaultPolicy;

  // auto add pid
  Dataset.observe("before save", (ctx, next) => {
    // console.log("Dataset.observe.beforeSave ctx", ctx);
    // console.log("Inside before save, ctx.data",JSON.stringify(ctx.data,null,3))
    // console.log("Inside before save, ctx.instance",JSON.stringify(ctx.instance,null,3))
    // console.log("Inside before save, ctx.currentinstance",JSON.stringify(ctx.currentInstance,null,3))
    // prevent recursion on auto history creation
    if (ctx.data && ctx.data.history) {
      delete ctx.data.updatedAt;
      delete ctx.data.updatedBy;
      return next();
    }
    if (ctx.instance) {
      if (ctx.isNewInstance) {
        ctx.instance.pid = config.pidPrefix + "/" + ctx.instance.pid;
        console.log("      New pid:", ctx.instance.pid);
        /* fill default datasetlifecycle
                    warning: need to transfer datasetlifecycle to a normal object first,
                    otherwise key tests give wrong results due to some "wrapping" of
                    the objects behind functions in loopback magic
                */
        var subblock = {};
        if (ctx.instance.datasetlifecycle) {
          subblock = JSON.parse(
            JSON.stringify(ctx.instance.datasetlifecycle)
          );
        } else {
          ctx.instance.datasetlifecycle = {};
        }
        if (!("archivable" in subblock))
          ctx.instance.datasetlifecycle.archivable = true;
        if (!("retrievable" in subblock))
          ctx.instance.datasetlifecycle.retrievable = false;
        if (!("publishable" in subblock))
          ctx.instance.datasetlifecycle.publishable = false;
        if (!("isOnCentralDisk" in subblock))
          ctx.instance.datasetlifecycle.isOnCentralDisk = true;
        if (!("archiveStatusMessage" in subblock))
          ctx.instance.datasetlifecycle.archiveStatusMessage =
                        "datasetCreated";
        if (!("retrieveStatusMessage" in subblock))
          ctx.instance.datasetlifecycle.retrieveStatusMessage = "";
        if (!("retrieveIntegrityCheck" in subblock))
          ctx.instance.datasetlifecycle.retrieveIntegrityCheck = false;
        // auto fill retention and publishing time
        var now = new Date();
        if (!ctx.instance.datasetlifecycle.archiveRetentionTime) {
          var retention = new Date(
            now.setFullYear(
              now.getFullYear() +
                                config.policyRetentionShiftInYears
            )
          );
          ctx.instance.datasetlifecycle.archiveRetentionTime = retention
            .toISOString()
            .substring(0, 10);
        }
        if (!ctx.instance.datasetlifecycle.dateOfPublishing) {
          now = new Date(); // now was modified above
          var pubDate = new Date(
            now.setFullYear(
              now.getFullYear() +
                                config.policyPublicationShiftInYears
            )
          );
          ctx.instance.datasetlifecycle.dateOfPublishing = pubDate
            .toISOString()
            .substring(0, 10);
        }
      } else {
        console.log("      Existing pid:", ctx.instance.pid);
      }
      ctx.instance.version = p.version;

      // sourceFolder handling
      if (ctx.instance.sourceFolder) {
        // remove trailing slashes
        if (ctx.instance.sourceFolder !== "/") {
          ctx.instance.sourceFolder = ctx.instance.sourceFolder.replace(/\/$/,"");
        }
        // autofill datasetName
        if (!ctx.instance.datasetName) {
          var arr = ctx.instance.sourceFolder.split("/");
          if (arr.length == 1) {
            ctx.instance.datasetName = arr[0];
          } else {
            ctx.instance.datasetName =
                            arr[arr.length - 2] + "/" + arr[arr.length - 1];
          }
        }
      }

      // auto fill classification and add policy if missing

      var Policy = app.models.Policy;
      const filter = {
        where: {
          ownerGroup: ctx.instance.ownerGroup
        }
      };
      Policy.findOne(filter, ctx.options, function(err, policyInstance) {
        if (err) {
          var msg =
                        "Error when looking for Policy of pgroup " +
                        ctx.instance.ownerGroup +
                        " " +
                        err;
          console.log(msg);
          next(msg);
        } else if (policyInstance) {
          if (!ctx.instance.classification) {
            // Case 1: classification undefined but policy defined:, define classification via policy
            let classification = "";
            switch (policyInstance.tapeRedundancy) {
            case "low":
              classification = "IN=medium,AV=low,CO=low";
              break;
            case "medium":
              classification = "IN=medium,AV=medium,CO=low";
              break;
            case "high":
              classification = "IN=medium,AV=high,CO=low";
              break;
            default:
              classification = "IN=medium,AV=low,CO=low";
            }
            ctx.instance.classification = classification;
          }
          // case 2: classification defined and policy defined: do nothing
          utils.keepHistory(ctx, next);
        } else {
          let tapeRedundancy = "low";
          if (!ctx.instance.classification) {
            // case 3: neither a policy nor a classification exist: define default classification and create default policy
            ctx.instance.classification = "IN=medium,AV=low,CO=low";
          } else {
            // case 4: classification exists but no policy: create policy from classification
            let classification = ctx.instance.classification;
            if (classification.includes("AV=low")) {
              tapeRedundancy = "low";
            } else if (classification.includes("AV=medium")) {
              tapeRedundancy = "medium";
            } else if (classification.includes("AV=high")) {
              tapeRedundancy = "high";
            }
          }
          addDefaultPolicy(
            ctx.instance.ownerGroup,
            ctx.instance.accessGroups,
            ctx.instance.ownerEmail,
            tapeRedundancy,
            ctx,
            next
          );
        }
      });

      // convert scientificMetadata value to SI
      if (ctx.instance.scientificMetadata) {
        const { scientificMetadata } = ctx.instance;
        utils.appendSIUnitToPhysicalQuantity(scientificMetadata);
      }
    } else {
      // update case
      utils.keepHistory(ctx, next);
    }
  });

  // clean up data connected to a dataset, e.g. if archiving failed
  // TODO can the additional findbyId calls be avoided ?

  Dataset.reset = function(id, options, next) {
    var Datablock = app.models.Datablock;
    Dataset.findById(id, options, function(err, l) {
      if (err) {
        next(err);
      } else {
        l.updateAttributes(
          {
            datasetlifecycle: {
              archivable: true,
              retrievable: false,
              publishable: false,
              archiveStatusMessage: "datasetCreated",
              retrieveStatusMessage: "",
              retrieveIntegrityCheck: false
            },
            packedSize: 0
          },
          options,
          function(err, _dsInstance) {
            if (err) {
              next(err);
            }
            Datablock.destroyAll(
              {
                datasetId: id
              },
              options,
              function(err, _b) {
                if (err) {
                  next(err);
                } else {
                  next();
                }
              }
            );
          }
        );
      }
    });
  };

  Dataset.isValid = function(dataset, next) {
    var ds = new Dataset(dataset);
    ds.isValid(function(valid) {
      if (!valid) {
        next(null, {
          errors: ds.errors,
          valid: false
        });
      } else {
        next(null, {
          valid: true
        });
      }
    });
  };

  Dataset.thumbnail = async function(id) {
    const Attachment = app.models.Attachment;
    const filter = {
      where: {
        datasetId: id
      }
    };
    return Attachment.findOne(filter).then(instance => {
      const base64stringExample =
                "data:image/png;base64,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";
      let base64string2 = "";
      if (instance && instance.__data && instance.__data.thumbnail !== undefined) {
        base64string2 = instance.__data.thumbnail;
      } else {
        base64string2 = base64stringExample;
      }
      return base64string2;
    });
  };

  /**
     * Produces a Kafka message for Dataset reduction in OpenWhisk, then consumes the response
     * @param {Dataset} dataset The Dataset to send for reduction
     * @returns {Object} The response from the OpenWhisk reduce action
     */

  Dataset.reduceDataset = function(dataset) {
    if (config.datasetReductionEnabled) {
      const Producer = kafka.Producer;
      const Consumer = kafka.Consumer;

      const client = new kafka.KafkaClient({
        kafkaHost: config.reductionKafkaBroker
      });
      const producer = new Producer(client);
      const consumer = new Consumer(client, [
        {
          topic: config.reductionKafkaOutputTopic,
          partition: 0
        }
      ]);

      const payloads = [
        {
          topic: config.reductionKafkaInputTopic,
          messages: JSON.stringify({
            datasetPid: dataset.pid
          }),
          partition: 0
        }
      ];

      return new Promise((resolve, reject) => {
        producer
          .on("ready", () => {
            producer.send(payloads, (err, data) => {
              if (err) {
                console.error(err);
                return reject(err);
              }
              console.log(
                "Produce to Kafka `{ topic: { partition: offset } }`: ",
                data
              );
            });
          })
          .on("error", err => {
            console.error(err);
          });

        consumer
          .on("message", message => {
            return resolve(JSON.parse(message.value));
          })
          .on("error", err => {
            console.error(err);
            return reject(err);
          });
      }).catch(err => {
        console.error(err);
      });
    }
  };

  Dataset.metadataKeys = async function(fields, limits, options) {
    try {
      const blacklist = [
        new RegExp(".*_date"),
        new RegExp("runNumber"),
        new RegExp("Entrych*."),
        new RegExp("entryCh*."),
        new RegExp("FMC-PICO*."),
        new RegExp("BW_measurement*."),
        new RegExp("Linearity_measurement*."),
        new RegExp("Pulse_measurement*.")
      ];
      const returnLimit = config.metadataKeysReturnLimit;
      const { metadataKey } = fields;

      // ensure that no more than MAXLIMIT someCollections are read for metadata key extraction
      let MAXLIMIT;
      if(config.metadataParentInstancesReturnLimit) {
        MAXLIMIT = config.metadataParentInstancesReturnLimit;

        let lm;

        if (limits) {
          lm = JSON.parse(JSON.stringify(limits));
        } else {
          lm = {};
        }

        if (lm.limit) {
          if (lm.limit > MAXLIMIT) {
            lm.limit = MAXLIMIT;
          }
        } else {
          lm.limit = MAXLIMIT;
        }
        limits = lm;
      }

      logger.logInfo("Fetching metadataKeys", {
        fields,
        limits,
        options,
        blacklist: blacklist.map(item => item.toString()),
        returnLimit
      });

      let someCollections;
      try {
        someCollections = await new Promise((resolve, reject) => {
          // TODO Is it okay to replace Dataset by MongoQueryableModel
          Dataset.fullquery(fields, limits, options, (err, res) => {
            if (err) {
              return reject(err);
            }
            resolve(res);
          });
        });
      } catch (err) {
        logger.logError(err.message, {
          location: "Dataset.metadataKeys.someCollections",
          fields,
          limits,
          options
        });
      }

      if (someCollections.length > 0) {
        logger.logInfo("Found someCollections", { count: someCollections.length });
      } else {
        logger.logInfo("No someCollections found", { someCollections });
      }

      const metadataKeys = utils.extractMetadataKeys(someCollections).filter(key => !blacklist.some(regex => regex.test(key)));

      logger.logInfo("Curated metadataKeys", {
        count: metadataKeys.length
      });

      if (metadataKey && metadataKey.length > 0) {
        const filterKey = metadataKey.toLowerCase();
        return metadataKeys
          .filter(key => key.toLowerCase().includes(filterKey))
          .slice(0, returnLimit);
      } else {
        return metadataKeys.slice(0, returnLimit);
      }
    } catch (err) {
      logger.logError(err.message, { location: "Dataset.metadatakeys" });
    }
  };
};
